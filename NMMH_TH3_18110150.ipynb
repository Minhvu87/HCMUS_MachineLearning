{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_DtPuBUEORG"
   },
   "source": [
    "#Logistic Regresion\n",
    "Ta định nghĩa với mọi $t \\in R$ thì hàm sigmoid được định nghĩa như sau:\n",
    "$$f(t)=\\frac{1}{1+\\epsilon^{-t}}$$\n",
    "$x_i \\in R^{n \\times 1}$ là một sample thứ $i$ trong tập dữ liệu,  $y_i \\in R^{1 \\times 1}$ là class của sample thứ $i$, $W \\in R^{n \\times 1}$ là trọng số mà ta cần tìm, ta có:\n",
    "\n",
    "$$\\hat{y_i}=f(x_i^TW)$$\n",
    "\n",
    "Ta định nghĩa hàm Loss như sau:\n",
    "\n",
    "$$Loss=\\frac{1}{N}\\sum^N-Ylog[f(X^TW)]-(1-Y)log[1-f(X^TW)]$$\n",
    "\n",
    "Với $X \\in R^{n \\times 1}$, $Y \\in R^{n \\times 1}$, $W \\in R^{n \\times 1}$.\n",
    "\n",
    "Đặt $Z=f(X^TW)$, ta có\n",
    "$$\\nabla_W Loss = -\\frac{1}{N} \\sum^N(\\frac{Y}{Z}-\\frac{1-Y}{1-Z})\\frac{\\partial Z}{\\partial W}$$\n",
    "\n",
    "Mà: $\\frac{\\partial Z}{\\partial W}=Z(1-Z)X$ nên:\n",
    "\n",
    "$$\\nabla_W Loss = -\\frac{1}{N} \\sum^N(Y-Z)X$$\n",
    "\n",
    "suy ra: \n",
    "$$W:=W-lr\\frac{1}{N} \\sum^N(Z-Y)X$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiP_kwLj9fXN"
   },
   "source": [
    "#Softmax Regression\n",
    "Ta hàm softmax như sau:\n",
    "$$a_i=\\frac{e^{X^Tw_i}}{\\sum_{j=1}^Ce^{X^Tw_i}}$$\n",
    "\n",
    "Với $w_i$ chính là trọng số cho hàm softmax của class thứ $i$. Nghĩa là với class thứ $i$ ta tương ứng có $a_i$ là hàm dự đoán xác xuất để sample $x_i$ rơi vào class này. $W=[w_1,w_2,...,w_C]$ là ma trận trọng số cần tìm, $W \\in R^{n \\times C}$, Với C là số Classes có trong dữ liệu\n",
    "\n",
    "Ngoài ta ta phải đổi y từ dạng scaler sang vector theo onehot encoding, tức là:\n",
    "$$y=[y_1,y_2,...,y_C]$$ \n",
    "với $\\sum_{i=1}^Cy_i=1$\n",
    "\n",
    "\n",
    "Ta định nghĩa hàm Loss như sau:\n",
    "\n",
    "$$Loss=\\frac{1}{N} \\sum^N(-\\sum_{i=1}^C y_i log(\\frac{e^{x^Tw_i}}{\\sum_j^Ce^{x^Tw_j}})) $$\n",
    "\n",
    "$$Loss=\\frac{1}{N} \\sum^N(-\\sum_{i=1}^C (y_ix^Tw_i-y_ilog(\\sum_j^Ce^{x^Tw_j}))) $$\n",
    "\n",
    "$$Loss=\\frac{1}{N} \\sum^N(-\\sum_{i=1}^C (y_ix^Tw_i) +log(\\sum_j^Ce^{x^Tw_j})) $$\n",
    "\n",
    "Gradient:\n",
    "$$\\nabla_W Loss = [\\frac{\\partial Loss}{\\partial w_1}, \\frac{\\partial Loss}{\\partial w_2},...,\\frac{\\partial Loss}{\\partial w_C}]$$\n",
    "\n",
    "Với $\\frac{\\partial Loss}{\\partial w_i}=\\frac{1}{N} \\sum^N(-y_i+\\frac{e^{x^Tw_i}}{\\sum_j^Ce^{x^Tw_j}})x$\n",
    "\n",
    "Từ đây ta có công thức cập nhật:\n",
    "\n",
    "$$W:= W- lr\\nabla_W Loss $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UpfabBedEKO8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBoTD6c7upP8"
   },
   "source": [
    "- Data này là dùng các features như tuổi, giới tính, lượng cholesterol để dự đoán bệnh nhân có bị mắc bệnh tim mạch hay không.\n",
    "\n",
    "- target gồm 2 label 1 và 0 tương ứng là mắc bệnh hay không mắc bệnh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5z4FCnqAmtE"
   },
   "source": [
    "#Bài tập"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8kP3tKp_IxN"
   },
   "source": [
    "1. Hãy xây dựng mô hình logistic regression bằng tất cả các features trong file heart, so sánh với thư viện sklearn.\n",
    "2. Hãy xây dựng mô hình softmax regression trên bộ Iris (nên Normalize data), so sánh với thư viện sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bài 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0     63    1   3       145   233    1        0      150      0      2.3   \n",
       "1     37    1   2       130   250    0        1      187      0      3.5   \n",
       "2     41    0   1       130   204    0        0      172      0      1.4   \n",
       "3     56    1   1       120   236    0        1      178      0      0.8   \n",
       "4     57    0   0       120   354    0        1      163      1      0.6   \n",
       "..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
       "298   57    0   0       140   241    0        1      123      1      0.2   \n",
       "299   45    1   3       110   264    0        1      132      0      1.2   \n",
       "300   68    1   0       144   193    1        1      141      0      3.4   \n",
       "301   57    1   0       130   131    0        1      115      1      1.2   \n",
       "302   57    0   1       130   236    0        0      174      0      0.0   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "0        0   0     1       1  \n",
       "1        0   0     2       1  \n",
       "2        2   0     2       1  \n",
       "3        2   0     2       1  \n",
       "4        2   0     2       1  \n",
       "..     ...  ..   ...     ...  \n",
       "298      1   0     3       0  \n",
       "299      1   0     3       0  \n",
       "300      1   2     3       0  \n",
       "301      1   1     3       0  \n",
       "302      1   1     2       0  \n",
       "\n",
       "[303 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"heart.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('target',axis = 1).values\n",
    "#X = df.loc[:,'chol'].values.reshape(df.shape[0],-1)\n",
    "y = data.loc[:,'target'].values.reshape(data.shape[0],-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,\n",
    "                                                    random_state=42)\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "min = np.min(X_train, axis=0)\n",
    "max = np.max(X_train, axis=0)\n",
    "X_train = (X_train-min) / (max -min)\n",
    "\n",
    "X_test = (X_test-min) / (max-min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hàm sigmoid\n",
    "def sigmoid(W,X):\n",
    "  \"\"\"\n",
    "  W là trọng số\n",
    "  X là sample(s)\n",
    "  \"\"\"\n",
    "  return 1/(1+np.exp((-X.dot(W))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logictisRegression(X, y, learning_rate, epoch):\n",
    "    #W_init=np.random.rand(14,1)\n",
    "    #W_init=np.array([[ 2.23789173],\n",
    "    #       [-0.96136103]]) #Đây là khởi tạo\n",
    "    Xbar = np.concatenate([np.ones([X.shape[0],1]),X],axis = 1)\n",
    "    W = np.zeros([Xbar.shape[1],1])\n",
    "    \"\"\"W_init=np.array([[ 0.38652764],\n",
    "           [-1.0147613 ]]) #lưu ý đây là W_init gần với W cần tìm để chạy cho nhanh\n",
    "           \"\"\"\n",
    "    bestLoss = 1e9\n",
    "    bestW = W\n",
    "    preLoss = 1e9\n",
    "    iStop = -1\n",
    "    for i in range(epoch):\n",
    "        prediction = sigmoid(W,Xbar).reshape(y.shape[0],-1) # Dự đoán\n",
    "\n",
    "        error = np.matmul(Xbar.T,prediction - y).reshape(W.shape[0],-1) #Tính error\n",
    "\n",
    "        gradient = 1/Xbar.shape[0]*error #Tính Gradient\n",
    "        preShape = W.shape\n",
    "        W = W - learning_rate * gradient #cập nhật W\n",
    "        loss = np.mean(-y*np.log(prediction)-(1-y)*np.log(1-prediction))\n",
    "        if(i%1000 == 0):\n",
    "            print(\"Loss at epoch {}: {}\".format(i,loss))\n",
    "        # Cap nhat W tot nhat\n",
    "        if(bestLoss > loss):\n",
    "            bestLoss = loss\n",
    "            bestW = W\n",
    "        # Thay doi qua it sau 1000 epoch thi thoat\n",
    "        if(abs(loss-preLoss)<1e-5):\n",
    "            if(i-iStop >= 1000):\n",
    "                break\n",
    "        else:\n",
    "            preLoss = loss\n",
    "            iStop = i\n",
    "    return bestW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0: 0.6931471805599452\n",
      "Loss at epoch 1000: 0.68964080036551\n",
      "Loss at epoch 2000: 0.6861905299088603\n",
      "Loss at epoch 3000: 0.6827945207767044\n",
      "Loss at epoch 4000: 0.6794511198519207\n",
      "Loss at epoch 5000: 0.6761588374658\n",
      "Loss at epoch 6000: 0.6729163209376794\n",
      "Loss at epoch 7000: 0.6697223326018695\n",
      "Loss at epoch 8000: 0.6665757315695962\n",
      "Loss at epoch 9000: 0.6634754585977738\n",
      "Loss at epoch 10000: 0.6604205235404841\n",
      "Loss at epoch 11000: 0.6574099949461603\n",
      "Loss at epoch 12000: 0.6544429914363821\n",
      "Loss at epoch 13000: 0.651518674563121\n",
      "Loss at epoch 14000: 0.6486362428921462\n",
      "Loss at epoch 15000: 0.6457949271027799\n",
      "Loss at epoch 16000: 0.6429939859295917\n",
      "Loss at epoch 17000: 0.6402327028011338\n",
      "Loss at epoch 18000: 0.6375103830554035\n",
      "Loss at epoch 19000: 0.634826351632173\n",
      "Loss at epoch 20000: 0.6321799511593531\n",
      "Loss at epoch 21000: 0.6295705403647152\n",
      "Loss at epoch 22000: 0.6269974927560542\n",
      "Loss at epoch 23000: 0.6244601955226626\n",
      "Loss at epoch 24000: 0.6219580486190871\n",
      "Loss at epoch 25000: 0.6194904639988943\n",
      "Loss at epoch 26000: 0.6170568649717475\n",
      "Loss at epoch 27000: 0.6146566856617443\n",
      "Loss at epoch 28000: 0.6122893705488001\n",
      "Loss at epoch 29000: 0.6099543740780531\n",
      "Loss at epoch 30000: 0.6076511603249023\n",
      "Loss at epoch 31000: 0.6053792027054735\n",
      "Loss at epoch 32000: 0.603137983724117\n",
      "Loss at epoch 33000: 0.6009269947510352\n",
      "Loss at epoch 34000: 0.5987457358243746\n",
      "Loss at epoch 35000: 0.5965937154721375\n",
      "Loss at epoch 36000: 0.5944704505501125\n",
      "Loss at epoch 37000: 0.5923754660927186\n",
      "Loss at epoch 38000: 0.5903082951742312\n",
      "Loss at epoch 39000: 0.588268478778333\n",
      "Loss at epoch 40000: 0.5862555656743212\n",
      "Loss at epoch 41000: 0.5842691122986221\n",
      "Loss at epoch 42000: 0.5823086826405265\n",
      "Loss at epoch 43000: 0.5803738481312818\n",
      "Loss at epoch 44000: 0.5784641875358437\n",
      "Loss at epoch 45000: 0.5765792868467425\n",
      "Loss at epoch 46000: 0.5747187391796346\n",
      "Loss at epoch 47000: 0.5728821446702079\n",
      "Loss at epoch 48000: 0.5710691103721849\n",
      "Loss at epoch 49000: 0.5692792501562319\n",
      "Loss at epoch 50000: 0.5675121846096385\n",
      "Loss at epoch 51000: 0.565767540936662\n",
      "Loss at epoch 52000: 0.5640449528594783\n",
      "Loss at epoch 53000: 0.5623440605196921\n",
      "Loss at epoch 54000: 0.5606645103803918\n",
      "Loss at epoch 55000: 0.5590059551287443\n",
      "Loss at epoch 56000: 0.5573680535791367\n",
      "Loss at epoch 57000: 0.5557504705768843\n",
      "Loss at epoch 58000: 0.5541528769025299\n",
      "Loss at epoch 59000: 0.5525749491767636\n",
      "Loss at epoch 60000: 0.5510163697659939\n",
      "Loss at epoch 61000: 0.549476826688607\n",
      "Loss at epoch 62000: 0.5479560135219502\n",
      "Loss at epoch 63000: 0.5464536293100716\n",
      "Loss at epoch 64000: 0.5449693784722556\n",
      "Loss at epoch 65000: 0.5435029707123844\n",
      "Loss at epoch 66000: 0.5420541209291615\n",
      "Loss at epoch 67000: 0.5406225491272257\n",
      "Loss at epoch 68000: 0.539207980329187\n",
      "Loss at epoch 69000: 0.5378101444886088\n",
      "Loss at epoch 70000: 0.5364287764039656\n",
      "Loss at epoch 71000: 0.5350636156335954\n",
      "Loss at epoch 72000: 0.5337144064116703\n",
      "Loss at epoch 73000: 0.5323808975652036\n",
      "Loss at epoch 74000: 0.5310628424321079\n",
      "Loss at epoch 75000: 0.5297599987803245\n",
      "Loss at epoch 76000: 0.5284721287280301\n",
      "Loss at epoch 77000: 0.5271989986649375\n",
      "Loss at epoch 78000: 0.5259403791746949\n",
      "Loss at epoch 79000: 0.5246960449583951\n",
      "Loss at epoch 80000: 0.5234657747591974\n",
      "Loss at epoch 81000: 0.522249351288068\n",
      "Loss at epoch 82000: 0.5210465611506406\n",
      "Loss at epoch 83000: 0.5198571947751993\n",
      "Loss at epoch 84000: 0.5186810463417824\n",
      "Loss at epoch 85000: 0.5175179137124085\n",
      "Loss at epoch 86000: 0.5163675983624192\n",
      "Loss at epoch 87000: 0.5152299053129371\n",
      "Loss at epoch 88000: 0.5141046430644333\n",
      "Loss at epoch 89000: 0.5129916235314\n",
      "Loss at epoch 90000: 0.5118906619781214\n",
      "Loss at epoch 91000: 0.5108015769555354\n",
      "Loss at epoch 92000: 0.5097241902391779\n",
      "Loss at epoch 93000: 0.5086583267682031\n",
      "Loss at epoch 94000: 0.5076038145854681\n",
      "Loss at epoch 95000: 0.5065604847786732\n",
      "Loss at epoch 96000: 0.5055281714225482\n",
      "Loss at epoch 97000: 0.5045067115220724\n",
      "Loss at epoch 98000: 0.5034959449567187\n",
      "Loss at epoch 99000: 0.5024957144257086\n",
      "Loss at epoch 100000: 0.5015058653942677\n",
      "Loss at epoch 101000: 0.5005262460408688\n",
      "Loss at epoch 102000: 0.49955670720544976\n",
      "Loss at epoch 103000: 0.49859710233859333\n",
      "Loss at epoch 104000: 0.49764728745165726\n",
      "Loss at epoch 105000: 0.49670712106783993\n",
      "Loss at epoch 106000: 0.49577646417416954\n",
      "Loss at epoch 107000: 0.49485518017440205\n",
      "Loss at epoch 108000: 0.49394313484281666\n",
      "Loss at epoch 109000: 0.4930401962788921\n",
      "Loss at epoch 110000: 0.4921462348628535\n",
      "Loss at epoch 111000: 0.4912611232120742\n",
      "Loss at epoch 112000: 0.49038473613831773\n",
      "Loss at epoch 113000: 0.4895169506058104\n",
      "Loss at epoch 114000: 0.48865764569012793\n",
      "Loss at epoch 115000: 0.4878067025378798\n",
      "Loss at epoch 116000: 0.4869640043271858\n",
      "Loss at epoch 117000: 0.4861294362289225\n",
      "Loss at epoch 118000: 0.48530288536873345\n",
      "Loss at epoch 119000: 0.48448424078978364\n",
      "Loss at epoch 120000: 0.48367339341625093\n",
      "Loss at epoch 121000: 0.4828702360175363\n",
      "Loss at epoch 122000: 0.4820746631731829\n",
      "Loss at epoch 123000: 0.48128657123849045\n",
      "Loss at epoch 124000: 0.48050585831081155\n",
      "Loss at epoch 125000: 0.4797324241965187\n",
      "Loss at epoch 126000: 0.4789661703786277\n",
      "Loss at epoch 127000: 0.478206999985067\n",
      "Loss at epoch 128000: 0.4774548177575789\n",
      "Loss at epoch 129000: 0.4767095300212424\n",
      "Loss at epoch 130000: 0.47597104465460566\n",
      "Loss at epoch 131000: 0.475239271060414\n",
      "Loss at epoch 132000: 0.4745141201369269\n",
      "Loss at epoch 133000: 0.4737955042498051\n",
      "Loss at epoch 134000: 0.4730833372045646\n",
      "Loss at epoch 135000: 0.47237753421958156\n",
      "Loss at epoch 136000: 0.4716780118996394\n",
      "Loss at epoch 137000: 0.47098468821000417\n",
      "Loss at epoch 138000: 0.47029748245102365\n",
      "Loss at epoch 139000: 0.4696163152332333\n",
      "Loss at epoch 140000: 0.4689411084529634\n",
      "Loss at epoch 141000: 0.46827178526843527\n",
      "Loss at epoch 142000: 0.4676082700763381\n",
      "Loss at epoch 143000: 0.46695048848887394\n",
      "Loss at epoch 144000: 0.4662983673112653\n",
      "Loss at epoch 145000: 0.4656518345197139\n",
      "Loss at epoch 146000: 0.4650108192397991\n",
      "Loss at epoch 147000: 0.46437525172531324\n",
      "Loss at epoch 148000: 0.46374506333751725\n",
      "Loss at epoch 149000: 0.4631201865248155\n",
      "Loss at epoch 150000: 0.4625005548028347\n",
      "Loss at epoch 151000: 0.46188610273490305\n",
      "Loss at epoch 152000: 0.4612767659129195\n",
      "Loss at epoch 153000: 0.4606724809386058\n",
      "Loss at epoch 154000: 0.46007318540513237\n",
      "Loss at epoch 155000: 0.45947881787911127\n",
      "Loss at epoch 156000: 0.45888931788294884\n",
      "Loss at epoch 157000: 0.4583046258775494\n",
      "Loss at epoch 158000: 0.45772468324536375\n",
      "Loss at epoch 159000: 0.45714943227377497\n",
      "Loss at epoch 160000: 0.45657881613881507\n",
      "Loss at epoch 161000: 0.45601277888920366\n",
      "Loss at epoch 162000: 0.4554512654307044\n",
      "Loss at epoch 163000: 0.4548942215107923\n",
      "Loss at epoch 164000: 0.4543415937036223\n",
      "Loss at epoch 165000: 0.453793329395298\n",
      "Loss at epoch 166000: 0.4532493767694289\n",
      "Loss at epoch 167000: 0.4527096847929746\n",
      "Loss at epoch 168000: 0.4521742032023676\n",
      "Loss at epoch 169000: 0.45164288248990836\n",
      "Loss at epoch 170000: 0.45111567389042967\n",
      "Loss at epoch 171000: 0.45059252936822114\n",
      "Loss at epoch 172000: 0.4500734016042123\n",
      "Loss at epoch 173000: 0.4495582439834041\n",
      "Loss at epoch 174000: 0.44904701058254953\n",
      "Loss at epoch 175000: 0.44853965615807223\n",
      "Loss at epoch 176000: 0.44803613613422266\n",
      "Loss at epoch 177000: 0.4475364065914639\n",
      "Loss at epoch 178000: 0.44704042425508445\n",
      "Loss at epoch 179000: 0.4465481464840317\n",
      "Loss at epoch 180000: 0.4460595312599623\n",
      "Loss at epoch 181000: 0.4455745371765047\n",
      "Loss at epoch 182000: 0.44509312342872925\n",
      "Loss at epoch 183000: 0.444615249802822\n",
      "Loss at epoch 184000: 0.4441408766659577\n",
      "Loss at epoch 185000: 0.44366996495636774\n",
      "Loss at epoch 186000: 0.4432024761735983\n",
      "Loss at epoch 187000: 0.44273837236895763\n",
      "Loss at epoch 188000: 0.442277616136144\n",
      "Loss at epoch 189000: 0.441820170602055\n",
      "Loss at epoch 190000: 0.4413659994177706\n",
      "Loss at epoch 191000: 0.4409150667497104\n",
      "Loss at epoch 192000: 0.4404673372709571\n",
      "Loss at epoch 193000: 0.4400227761527466\n",
      "Loss at epoch 194000: 0.4395813490561186\n",
      "Loss at epoch 195000: 0.4391430221237262\n",
      "Loss at epoch 196000: 0.43870776197179967\n",
      "Loss at epoch 197000: 0.43827553568226224\n",
      "Loss at epoch 198000: 0.43784631079499553\n",
      "Loss at epoch 199000: 0.4374200553002485\n",
      "Loss at epoch 200000: 0.43699673763119057\n",
      "Loss at epoch 201000: 0.4365763266566047\n",
      "Loss at epoch 202000: 0.43615879167371535\n",
      "Loss at epoch 203000: 0.43574410240115247\n",
      "Loss at epoch 204000: 0.43533222897204543\n",
      "Loss at epoch 205000: 0.434923141927247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 206000: 0.43451681220868027\n",
      "Loss at epoch 207000: 0.4341132111528138\n",
      "Loss at epoch 208000: 0.4337123104842522\n",
      "Loss at epoch 209000: 0.4333140823094494\n",
      "Loss at epoch 210000: 0.4329184991105357\n",
      "Loss at epoch 211000: 0.4325255337392595\n",
      "Loss at epoch 212000: 0.4321351594110395\n",
      "Loss at epoch 213000: 0.4317473496991267\n",
      "Loss at epoch 214000: 0.4313620785288733\n",
      "Loss at epoch 215000: 0.43097932017210605\n",
      "Loss at epoch 216000: 0.4305990492416023\n",
      "Loss at epoch 217000: 0.4302212406856668\n",
      "Loss at epoch 218000: 0.4298458697828068\n",
      "Loss at epoch 219000: 0.4294729121365045\n",
      "Loss at epoch 220000: 0.4291023436700827\n",
      "Loss at epoch 221000: 0.4287341406216645\n",
      "Loss at epoch 222000: 0.4283682795392233\n",
      "Loss at epoch 223000: 0.4280047372757225\n",
      "Loss at epoch 224000: 0.42764349098434135\n",
      "Loss at epoch 225000: 0.4272845181137877\n",
      "Loss at epoch 226000: 0.4269277964036938\n",
      "Loss at epoch 227000: 0.42657330388009457\n",
      "Loss at epoch 228000: 0.42622101885098634\n",
      "Loss at epoch 229000: 0.4258709199019649\n",
      "Loss at epoch 230000: 0.4255229858919409\n",
      "Loss at epoch 231000: 0.4251771959489308\n",
      "Loss at epoch 232000: 0.42483352946592257\n",
      "Loss at epoch 233000: 0.4244919660968143\n",
      "Loss at epoch 234000: 0.42415248575242487\n",
      "Loss at epoch 235000: 0.42381506859657325\n",
      "Loss at epoch 236000: 0.4234796950422282\n",
      "Loss at epoch 237000: 0.42314634574772475\n",
      "Loss at epoch 238000: 0.4228150016130464\n",
      "Loss at epoch 239000: 0.42248564377617276\n",
      "Loss at epoch 240000: 0.4221582536094906\n",
      "Loss at epoch 241000: 0.4218328127162668\n",
      "Loss at epoch 242000: 0.4215093029271833\n",
      "Loss at epoch 243000: 0.4211877062969309\n",
      "Loss at epoch 244000: 0.4208680051008627\n",
      "Loss at epoch 245000: 0.42055018183170495\n",
      "Loss at epoch 246000: 0.4202342191963243\n",
      "Loss at epoch 247000: 0.4199201001125502\n",
      "Loss at epoch 248000: 0.4196078077060533\n",
      "Loss at epoch 249000: 0.4192973253072748\n",
      "Loss at epoch 250000: 0.41898863644841033\n",
      "Loss at epoch 251000: 0.4186817248604433\n",
      "Loss at epoch 252000: 0.41837657447023024\n",
      "Loss at epoch 253000: 0.4180731693976347\n",
      "Loss at epoch 254000: 0.4177714939527098\n",
      "Loss at epoch 255000: 0.41747153263292763\n",
      "Loss at epoch 256000: 0.4171732701204574\n",
      "Loss at epoch 257000: 0.4168766912794867\n",
      "Loss at epoch 258000: 0.4165817811535897\n",
      "Loss at epoch 259000: 0.41628852496313806\n",
      "Loss at epoch 260000: 0.41599690810275625\n",
      "Loss at epoch 261000: 0.41570691613881866\n",
      "Loss at epoch 262000: 0.415418534806988\n",
      "Loss at epoch 263000: 0.41513175000979574\n",
      "Loss at epoch 264000: 0.41484654781426095\n",
      "Loss at epoch 265000: 0.4145629144495507\n",
      "Loss at epoch 266000: 0.4142808363046766\n",
      "Loss at epoch 267000: 0.4140002999262314\n",
      "Loss at epoch 268000: 0.41372129201616187\n",
      "Loss at epoch 269000: 0.413443799429578\n",
      "Loss at epoch 270000: 0.41316780917259827\n",
      "Loss at epoch 271000: 0.41289330840022986\n",
      "Loss at epoch 272000: 0.41262028441428517\n",
      "Loss at epoch 273000: 0.41234872466132877\n",
      "Loss at epoch 274000: 0.4120786167306609\n",
      "Loss at epoch 275000: 0.4118099483523324\n",
      "Loss at epoch 276000: 0.41154270739519144\n",
      "Loss at epoch 277000: 0.41127688186496164\n",
      "Loss at epoch 278000: 0.4110124599023522\n",
      "Loss at epoch 279000: 0.41074942978119816\n",
      "Loss at epoch 280000: 0.41048777990662905\n",
      "Loss at epoch 281000: 0.4102274988132681\n",
      "Loss at epoch 282000: 0.4099685751634601\n",
      "Loss at epoch 283000: 0.40971099774552827\n",
      "Loss at epoch 284000: 0.40945475547205623\n",
      "Loss at epoch 285000: 0.40919983737820004\n",
      "Loss at epoch 286000: 0.40894623262002566\n",
      "Loss at epoch 287000: 0.408693930472873\n",
      "Loss at epoch 288000: 0.4084429203297451\n",
      "Loss at epoch 289000: 0.4081931916997242\n",
      "Loss at epoch 290000: 0.40794473420641103\n",
      "Loss at epoch 291000: 0.40769753758638894\n",
      "Loss at epoch 292000: 0.4074515916877135\n",
      "Loss at epoch 293000: 0.40720688646842335\n",
      "Loss at epoch 294000: 0.4069634119950759\n",
      "Loss at epoch 295000: 0.4067211584413058\n",
      "Loss at epoch 296000: 0.40648011608640444\n",
      "Loss at epoch 297000: 0.406240275313923\n",
      "Loss at epoch 298000: 0.4060016266102957\n",
      "Loss at epoch 299000: 0.4057641605634857\n",
      "Loss at epoch 300000: 0.40552786786165074\n",
      "Loss at epoch 301000: 0.40529273929182946\n",
      "Loss at epoch 302000: 0.4050587657386485\n",
      "Loss at epoch 303000: 0.40482593818304835\n",
      "Loss at epoch 304000: 0.4045942477010295\n",
      "Loss at epoch 305000: 0.404363685462417\n",
      "Loss at epoch 306000: 0.4041342427296451\n",
      "Loss at epoch 307000: 0.4039059108565578\n",
      "Loss at epoch 308000: 0.40367868128723133\n",
      "Loss at epoch 309000: 0.40345254555480997\n",
      "Loss at epoch 310000: 0.4032274952803632\n",
      "Loss at epoch 311000: 0.40300352217175794\n",
      "Loss at epoch 312000: 0.4027806180225481\n",
      "Loss at epoch 313000: 0.4025587747108817\n",
      "Loss at epoch 314000: 0.4023379841984232\n",
      "Loss at epoch 315000: 0.40211823852929185\n",
      "Loss at epoch 316000: 0.40189952982901744\n",
      "Loss at epoch 317000: 0.40168185030350934\n",
      "Loss at epoch 318000: 0.40146519223804245\n",
      "Loss at epoch 319000: 0.4012495479962579\n",
      "Loss at epoch 320000: 0.4010349100191773\n",
      "Loss at epoch 321000: 0.400821270824234\n",
      "Loss at epoch 322000: 0.40060862300431577\n",
      "Loss at epoch 323000: 0.40039695922682295\n",
      "Loss at epoch 324000: 0.40018627223274145\n",
      "Loss at epoch 325000: 0.3999765548357269\n",
      "Loss at epoch 326000: 0.3997677999212038\n",
      "Loss at epoch 327000: 0.3995600004454782\n",
      "Loss at epoch 328000: 0.39935314943486094\n",
      "Loss at epoch 329000: 0.39914723998480645\n",
      "Loss at epoch 330000: 0.39894226525906246\n",
      "Loss at epoch 331000: 0.3987382184888318\n",
      "Loss at epoch 332000: 0.3985350929719474\n",
      "Loss at epoch 333000: 0.3983328820720578\n",
      "Loss at epoch 334000: 0.39813157921782555\n",
      "Loss at epoch 335000: 0.39793117790213656\n",
      "Loss at epoch 336000: 0.39773167168132095\n",
      "Loss at epoch 337000: 0.39753305417438406\n",
      "Loss at epoch 338000: 0.3973353190622502\n",
      "Loss at epoch 339000: 0.39713846008701603\n",
      "Loss at epoch 340000: 0.39694247105121433\n",
      "Loss at epoch 341000: 0.3967473458170889\n",
      "Loss at epoch 342000: 0.396553078305879\n",
      "Loss at epoch 343000: 0.3963596624971148\n",
      "Loss at epoch 344000: 0.39616709242792125\n",
      "Loss at epoch 345000: 0.39597536219233387\n",
      "Loss at epoch 346000: 0.3957844659406223\n",
      "Loss at epoch 347000: 0.3955943978786239\n",
      "Loss at epoch 348000: 0.395405152267087\n",
      "Loss at epoch 349000: 0.39521672342102254\n",
      "Loss at epoch 350000: 0.39502910570906574\n",
      "Loss at epoch 351000: 0.39484229355284634\n",
      "Loss at epoch 352000: 0.3946562814263663\n",
      "Loss at epoch 353000: 0.3944710638553876\n",
      "Loss at epoch 354000: 0.39428663541682896\n",
      "Loss at epoch 355000: 0.39410299073816807\n",
      "Loss at epoch 356000: 0.39392012449685604\n",
      "Loss at epoch 357000: 0.39373803141973573\n",
      "Loss at epoch 358000: 0.39355670628247136\n",
      "Loss at epoch 359000: 0.39337614390898423\n",
      "Loss at epoch 360000: 0.3931963391708963\n",
      "Loss at epoch 361000: 0.393017286986982\n",
      "Loss at epoch 362000: 0.3928389823226271\n",
      "Loss at epoch 363000: 0.3926614201892942\n",
      "Loss at epoch 364000: 0.39248459564399624\n",
      "Loss at epoch 365000: 0.39230850378877763\n",
      "Loss at epoch 366000: 0.39213313977020153\n",
      "Loss at epoch 367000: 0.39195849877884414\n",
      "Loss at epoch 368000: 0.3917845760487953\n",
      "Loss at epoch 369000: 0.39161136685716735\n",
      "Loss at epoch 370000: 0.3914388665236093\n",
      "Loss at epoch 371000: 0.39126707040982733\n",
      "Loss at epoch 372000: 0.39109597391911244\n",
      "Loss at epoch 373000: 0.39092557249587384\n",
      "Loss at epoch 374000: 0.39075586162517945\n",
      "Loss at epoch 375000: 0.3905868368323008\n",
      "Loss at epoch 376000: 0.3904184936822656\n",
      "Loss at epoch 377000: 0.3902508277794152\n",
      "Loss at epoch 378000: 0.3900838347669688\n",
      "Loss at epoch 379000: 0.3899175103265918\n",
      "Loss at epoch 380000: 0.38975185017797154\n",
      "Loss at epoch 381000: 0.38958685007839816\n",
      "Loss at epoch 382000: 0.38942250582234983\n",
      "Loss at epoch 383000: 0.38925881324108486\n",
      "Loss at epoch 384000: 0.3890957682022384\n",
      "Loss at epoch 385000: 0.3889333666094239\n",
      "Loss at epoch 386000: 0.3887716044018412\n",
      "Loss at epoch 387000: 0.3886104775538881\n",
      "Loss at epoch 388000: 0.38844998207477816\n",
      "Loss at epoch 389000: 0.3882901140081626\n",
      "Loss at epoch 390000: 0.388130869431757\n",
      "Loss at epoch 391000: 0.38797224445697404\n",
      "Loss at epoch 392000: 0.38781423522855957\n",
      "Loss at epoch 393000: 0.3876568379242339\n",
      "Loss at epoch 394000: 0.3875000487543378\n",
      "Loss at epoch 395000: 0.3873438639614829\n",
      "Loss at epoch 396000: 0.3871882798202058\n",
      "Loss at epoch 397000: 0.3870332926366281\n",
      "Loss at epoch 398000: 0.3868788987481199\n",
      "Loss at epoch 399000: 0.38672509452296633\n",
      "Loss at epoch 400000: 0.38657187636004126\n",
      "Loss at epoch 401000: 0.38641924068848227\n",
      "Loss at epoch 402000: 0.38626718396737086\n",
      "Loss at epoch 403000: 0.386115702685417\n",
      "Loss at epoch 404000: 0.3859647933606474\n",
      "Loss at epoch 405000: 0.3858144525400969\n",
      "Loss at epoch 406000: 0.3856646767995051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 407000: 0.38551546274301535\n",
      "Loss at epoch 408000: 0.38536680700287884\n",
      "Loss at epoch 409000: 0.3852187062391611\n",
      "Loss at epoch 410000: 0.3850711571394534\n",
      "Loss at epoch 411000: 0.3849241564185864\n",
      "Loss at epoch 412000: 0.3847777008183486\n",
      "Loss at epoch 413000: 0.3846317871072067\n",
      "Loss at epoch 414000: 0.3844864120800318\n",
      "Loss at epoch 415000: 0.3843415725578262\n",
      "Loss at epoch 416000: 0.3841972653874558\n",
      "Loss at epoch 417000: 0.38405348744138396\n",
      "Loss at epoch 418000: 0.3839102356174107\n",
      "Loss at epoch 419000: 0.3837675068384123\n",
      "Loss at epoch 420000: 0.38362529805208706\n",
      "Loss at epoch 421000: 0.3834836062307023\n",
      "Loss at epoch 422000: 0.3833424283708442\n",
      "Loss at epoch 423000: 0.38320176149317264\n",
      "Loss at epoch 424000: 0.38306160264217637\n",
      "Loss at epoch 425000: 0.3829219488859333\n",
      "Loss at epoch 426000: 0.38278279731587234\n",
      "Loss at epoch 427000: 0.3826441450465389\n",
      "Loss at epoch 428000: 0.382505989215363\n",
      "Loss at epoch 429000: 0.38236832698242934\n",
      "Loss at epoch 430000: 0.3822311555302523\n",
      "Loss at epoch 431000: 0.3820944720635499\n",
      "Loss at epoch 432000: 0.381958273809025\n",
      "Loss at epoch 433000: 0.3818225580151452\n",
      "Loss at epoch 434000: 0.3816873219519283\n",
      "Loss at epoch 435000: 0.38155256291072753\n",
      "Loss at epoch 436000: 0.3814182782040212\n",
      "Loss at epoch 437000: 0.3812844651652055\n",
      "Loss at epoch 438000: 0.3811511211483868\n",
      "Loss at epoch 439000: 0.38101824352817976\n",
      "Loss at epoch 440000: 0.3808858296995053\n",
      "Loss at epoch 441000: 0.3807538770773921\n",
      "Loss at epoch 442000: 0.3806223830967806\n",
      "Loss at epoch 443000: 0.3804913452123283\n",
      "Loss at epoch 444000: 0.3803607608982188\n",
      "Loss at epoch 445000: 0.38023062764797094\n",
      "Loss at epoch 446000: 0.3801009429742524\n",
      "Loss at epoch 447000: 0.37997170440869416\n",
      "Loss at epoch 448000: 0.37984290950170735\n",
      "Loss at epoch 449000: 0.37971455582230257\n",
      "Loss at epoch 450000: 0.37958664095791045\n",
      "Loss at epoch 451000: 0.37945916251420514\n",
      "Loss at epoch 452000: 0.3793321181149299\n",
      "Loss at epoch 453000: 0.3792055054017234\n",
      "Loss at epoch 454000: 0.3790793220339496\n",
      "Loss at epoch 455000: 0.3789535656885292\n",
      "Loss at epoch 456000: 0.3788282340597713\n",
      "Loss at epoch 457000: 0.37870332485921004\n",
      "Loss at epoch 458000: 0.37857883581544005\n",
      "Loss at epoch 459000: 0.37845476467395606\n",
      "Loss at epoch 460000: 0.378331109196993\n",
      "Loss at epoch 461000: 0.37820786716336874\n",
      "Loss at epoch 462000: 0.3780850363683274\n",
      "Loss at epoch 463000: 0.37796261462338576\n",
      "Loss at epoch 464000: 0.3778405997561807\n",
      "Loss at epoch 465000: 0.3777189896103189\n",
      "Loss at epoch 466000: 0.37759778204522704\n",
      "Loss at epoch 467000: 0.37747697493600485\n",
      "Loss at epoch 468000: 0.3773565661732799\n",
      "Loss at epoch 469000: 0.3772365536630625\n",
      "Loss at epoch 470000: 0.37711693532660423\n",
      "Loss at epoch 471000: 0.37699770910025604\n",
      "Loss at epoch 472000: 0.37687887293533007\n",
      "Loss at epoch 473000: 0.37676042479796074\n",
      "Loss at epoch 474000: 0.3766423626689692\n",
      "Loss at epoch 475000: 0.3765246845437285\n",
      "Loss at epoch 476000: 0.3764073884320301\n",
      "Loss at epoch 477000: 0.3762904723579517\n",
      "Loss at epoch 478000: 0.3761739343597279\n",
      "Loss at epoch 479000: 0.3760577724896205\n",
      "Loss at epoch 480000: 0.37594198481379076\n",
      "Loss at epoch 481000: 0.375826569412174\n",
      "Loss at epoch 482000: 0.37571152437835453\n",
      "Loss at epoch 483000: 0.3755968478194417\n",
      "Loss at epoch 484000: 0.37548253785594865\n",
      "Loss at epoch 485000: 0.3753685926216714\n",
      "Loss at epoch 486000: 0.3752550102635686\n",
      "Loss at epoch 487000: 0.37514178894164507\n",
      "Loss at epoch 488000: 0.37502892682883265\n",
      "Loss at epoch 489000: 0.3749164221108768\n",
      "Loss at epoch 490000: 0.37480427298622077\n",
      "Loss at epoch 491000: 0.374692477665893\n",
      "Loss at epoch 492000: 0.37458103437339535\n",
      "Loss at epoch 493000: 0.37446994134459166\n",
      "Loss at epoch 494000: 0.37435919682759905\n",
      "Loss at epoch 495000: 0.37424879908267905\n",
      "Loss at epoch 496000: 0.3741387463821305\n",
      "Loss at epoch 497000: 0.3740290370101837\n",
      "Loss at epoch 498000: 0.3739196692628947\n",
      "Loss at epoch 499000: 0.3738106414480425\n",
      "Loss at epoch 500000: 0.373701951885025\n",
      "Loss at epoch 501000: 0.3735935989047588\n",
      "Loss at epoch 502000: 0.3734855808495774\n",
      "Loss at epoch 503000: 0.37337789607313204\n",
      "Loss at epoch 504000: 0.37327054294029366\n",
      "Loss at epoch 505000: 0.37316351982705454\n",
      "Loss at epoch 506000: 0.37305682512043264\n",
      "Loss at epoch 507000: 0.3729504572183762\n",
      "Loss at epoch 508000: 0.3728444145296682\n",
      "Loss at epoch 509000: 0.37273869547383404\n",
      "Loss at epoch 510000: 0.37263329848104876\n",
      "Loss at epoch 511000: 0.37252822199204527\n",
      "Loss at epoch 512000: 0.3724234644580238\n",
      "Loss at epoch 513000: 0.37231902434056274\n",
      "Loss at epoch 514000: 0.3722149001115293\n",
      "Loss at epoch 515000: 0.372111090252992\n",
      "Loss at epoch 516000: 0.3720075932571333\n",
      "Loss at epoch 517000: 0.3719044076261645\n",
      "Loss at epoch 518000: 0.37180153187223997\n",
      "Loss at epoch 519000: 0.371698964517373\n",
      "Loss at epoch 520000: 0.37159670409335244\n",
      "Loss at epoch 521000: 0.37149474914166014\n",
      "Loss at epoch 522000: 0.3713930982133893\n",
      "Loss at epoch 523000: 0.37129174986916325\n",
      "Loss at epoch 524000: 0.3711907026790561\n",
      "Loss at epoch 525000: 0.37108995522251287\n",
      "Loss at epoch 526000: 0.3709895060882713\n",
      "Loss at epoch 527000: 0.37088935387428423\n",
      "Loss at epoch 528000: 0.37078949718764265\n",
      "Loss at epoch 529000: 0.3706899346444995\n",
      "Loss at epoch 530000: 0.37059066486999476\n",
      "Loss at epoch 531000: 0.3704916864981803\n",
      "Loss at epoch 532000: 0.3703929981719466\n",
      "Loss at epoch 533000: 0.37029459854294927\n",
      "Loss at epoch 534000: 0.37019648627153695\n",
      "Loss at epoch 535000: 0.3700986600266798\n",
      "Loss at epoch 536000: 0.3700011184858976\n",
      "Loss at epoch 537000: 0.3699038603351913\n",
      "Loss at epoch 538000: 0.36980688426897196\n",
      "Loss at epoch 539000: 0.3697101889899922\n",
      "Loss at epoch 540000: 0.3696137732092789\n",
      "Loss at epoch 541000: 0.369517635646065\n",
      "Loss at epoch 542000: 0.36942177502772244\n",
      "Loss at epoch 543000: 0.36932619008969697\n",
      "Loss at epoch 544000: 0.36923087957544154\n",
      "Loss at epoch 545000: 0.3691358422363527\n",
      "Loss at epoch 546000: 0.3690410768317044\n",
      "Loss at epoch 547000: 0.3689465821285873\n",
      "Loss at epoch 548000: 0.3688523569018428\n",
      "Loss at epoch 549000: 0.3687583999340025\n",
      "Loss at epoch 550000: 0.3686647100152262\n",
      "Loss at epoch 551000: 0.3685712859432403\n",
      "Loss at epoch 552000: 0.36847812652327716\n",
      "Loss at epoch 553000: 0.3683852305680161\n",
      "Loss at epoch 554000: 0.3682925968975227\n",
      "Loss at epoch 555000: 0.368200224339191\n",
      "Loss at epoch 556000: 0.3681081117276846\n",
      "Loss at epoch 557000: 0.3680162579048794\n",
      "Loss at epoch 558000: 0.3679246617198062\n",
      "Loss at epoch 559000: 0.36783332202859415\n",
      "Loss at epoch 560000: 0.3677422376944148\n",
      "Loss at epoch 561000: 0.36765140758742665\n",
      "Loss at epoch 562000: 0.3675608305847199\n",
      "Loss at epoch 563000: 0.36747050557026206\n",
      "Loss at epoch 564000: 0.3673804314348442\n",
      "Loss at epoch 565000: 0.36729060707602745\n",
      "Loss at epoch 566000: 0.36720103139809\n",
      "Loss at epoch 567000: 0.36711170331197446\n",
      "Loss at epoch 568000: 0.367022621735237\n",
      "Loss at epoch 569000: 0.36693378559199413\n",
      "Loss at epoch 570000: 0.3668451938128732\n",
      "Loss at epoch 571000: 0.3667568453349616\n",
      "Loss at epoch 572000: 0.3666687391017563\n",
      "Loss at epoch 573000: 0.366580874063115\n",
      "Loss at epoch 574000: 0.36649324917520637\n",
      "Loss at epoch 575000: 0.36640586340046216\n",
      "Loss at epoch 576000: 0.3663187157075286\n",
      "Loss at epoch 577000: 0.36623180507121844\n",
      "Loss at epoch 578000: 0.36614513047246455\n",
      "Loss at epoch 579000: 0.36605869089827225\n",
      "Loss at epoch 580000: 0.3659724853416735\n",
      "Loss at epoch 581000: 0.3658865128016807\n",
      "Loss at epoch 582000: 0.36580077228324115\n",
      "Loss at epoch 583000: 0.3657152627971918\n",
      "Loss at epoch 584000: 0.36562998336021496\n",
      "Loss at epoch 585000: 0.365544932994794\n",
      "Loss at epoch 586000: 0.36546011072916906\n",
      "Loss at epoch 587000: 0.36537551559729364\n",
      "Loss at epoch 588000: 0.36529114663879186\n",
      "Loss at epoch 589000: 0.36520700289891583\n",
      "Loss at epoch 590000: 0.36512308342850297\n",
      "Loss at epoch 591000: 0.3650393872839342\n",
      "Loss at epoch 592000: 0.3649559135270926\n",
      "Loss at epoch 593000: 0.3648726612253223\n",
      "Loss at epoch 594000: 0.3647896294513875\n",
      "Loss at epoch 595000: 0.36470681728343224\n",
      "Loss at epoch 596000: 0.36462422380494025\n",
      "Loss at epoch 597000: 0.3645418481046952\n",
      "Loss at epoch 598000: 0.3644596892767417\n",
      "Loss at epoch 599000: 0.3643777464203464\n",
      "Loss at epoch 600000: 0.3642960186399586\n",
      "Loss at epoch 601000: 0.3642145050451729\n",
      "Loss at epoch 602000: 0.36413320475069094\n",
      "Loss at epoch 603000: 0.36405211687628375\n",
      "Loss at epoch 604000: 0.3639712405467543\n",
      "Loss at epoch 605000: 0.36389057489190124\n",
      "Loss at epoch 606000: 0.36381011904648175\n",
      "Loss at epoch 607000: 0.36372987215017577\n",
      "Loss at epoch 608000: 0.3636498333475493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 609000: 0.3635700017880194\n",
      "Loss at epoch 610000: 0.36349037662581885\n",
      "Loss at epoch 611000: 0.3634109570199612\n",
      "Loss at epoch 612000: 0.3633317421342059\n",
      "Loss at epoch 613000: 0.3632527311370239\n",
      "Loss at epoch 614000: 0.36317392320156355\n",
      "Loss at epoch 615000: 0.3630953175056175\n",
      "Loss at epoch 616000: 0.363016913231588\n",
      "Loss at epoch 617000: 0.3629387095664551\n",
      "Loss at epoch 618000: 0.3628607057017423\n",
      "Loss at epoch 619000: 0.3627829008334854\n",
      "Loss at epoch 620000: 0.3627052941621995\n",
      "Loss at epoch 621000: 0.3626278848928466\n",
      "Loss at epoch 622000: 0.3625506722348043\n",
      "Loss at epoch 623000: 0.36247365540183485\n",
      "Loss at epoch 624000: 0.3623968336120526\n",
      "Loss at epoch 625000: 0.3623202060878942\n",
      "Loss at epoch 626000: 0.36224377205608754\n",
      "Loss at epoch 627000: 0.3621675307476215\n",
      "Loss at epoch 628000: 0.36209148139771496\n",
      "Loss at epoch 629000: 0.3620156232457889\n",
      "Loss at epoch 630000: 0.36193995553543445\n",
      "Loss at epoch 631000: 0.36186447751438505\n",
      "Loss at epoch 632000: 0.3617891884344867\n",
      "Loss at epoch 633000: 0.36171408755166934\n",
      "Loss at epoch 634000: 0.36163917412591834\n",
      "Loss at epoch 635000: 0.3615644474212456\n",
      "Loss at epoch 636000: 0.3614899067056618\n",
      "Loss at epoch 637000: 0.361415551251149\n",
      "Loss at epoch 638000: 0.36134138033363217\n",
      "Loss at epoch 639000: 0.36126739323295176\n",
      "Loss at epoch 640000: 0.3611935892328369\n",
      "Loss at epoch 641000: 0.36111996762087845\n",
      "Loss at epoch 642000: 0.36104652768850226\n",
      "Loss at epoch 643000: 0.36097326873094254\n",
      "Loss at epoch 644000: 0.36090019004721535\n",
      "Loss at epoch 645000: 0.360827290940093\n",
      "Loss at epoch 646000: 0.360754570716078\n",
      "Loss at epoch 647000: 0.3606820286853776\n",
      "Loss at epoch 648000: 0.36060966416187756\n",
      "Loss at epoch 649000: 0.3605374764631185\n",
      "Loss at epoch 650000: 0.36046546491026943\n",
      "Loss at epoch 651000: 0.3603936288281038\n",
      "Loss at epoch 652000: 0.3603219675449743\n",
      "Loss at epoch 653000: 0.36025048039278956\n",
      "Loss at epoch 654000: 0.36017916670698874\n",
      "Loss at epoch 655000: 0.36010802582651824\n",
      "Loss at epoch 656000: 0.36003705709380807\n",
      "Loss at epoch 657000: 0.35996625985474767\n",
      "Loss at epoch 658000: 0.3598956334586631\n",
      "Loss at epoch 659000: 0.3598251772582935\n",
      "Loss at epoch 660000: 0.3597548906097685\n",
      "Loss at epoch 661000: 0.35968477287258493\n",
      "Loss at epoch 662000: 0.35961482340958434\n",
      "Loss at epoch 663000: 0.3595450415869311\n",
      "Loss at epoch 664000: 0.3594754267740892\n",
      "Loss at epoch 665000: 0.3594059783438011\n",
      "Loss at epoch 666000: 0.3593366956720653\n",
      "Loss at epoch 667000: 0.35926757813811466\n",
      "Loss at epoch 668000: 0.3591986251243948\n",
      "Loss at epoch 669000: 0.35912983601654375\n",
      "Loss at epoch 670000: 0.3590612102033694\n",
      "Loss at epoch 671000: 0.35899274707682816\n",
      "Loss at epoch 672000: 0.35892444603200685\n",
      "Loss at epoch 673000: 0.3588563064670989\n",
      "Loss at epoch 674000: 0.3587883277833852\n",
      "Loss at epoch 675000: 0.3587205093852141\n",
      "Loss at epoch 676000: 0.35865285067998043\n",
      "Loss at epoch 677000: 0.35858535107810613\n",
      "Loss at epoch 678000: 0.35851800999302\n",
      "Loss at epoch 679000: 0.35845082684113816\n",
      "Loss at epoch 680000: 0.3583838010418446\n",
      "Loss at epoch 681000: 0.3583169320174714\n",
      "Loss at epoch 682000: 0.3582502191932801\n",
      "Loss at epoch 683000: 0.3581836619974421\n",
      "Loss at epoch 684000: 0.3581172598610203\n",
      "Loss at epoch 685000: 0.35805101221794944\n",
      "Loss at epoch 686000: 0.35798491850501857\n",
      "Loss at epoch 687000: 0.3579189781618518\n",
      "Loss at epoch 688000: 0.3578531906308902\n",
      "Loss at epoch 689000: 0.3577875553573733\n",
      "Loss at epoch 690000: 0.3577220717893218\n",
      "Loss at epoch 691000: 0.35765673937751913\n",
      "Loss at epoch 692000: 0.3575915575754937\n",
      "Loss at epoch 693000: 0.35752652583950173\n",
      "Loss at epoch 694000: 0.3574616436285088\n",
      "Loss at epoch 695000: 0.3573969104041738\n",
      "Loss at epoch 696000: 0.3573323256308303\n",
      "Loss at epoch 697000: 0.35726788877547083\n",
      "Loss at epoch 698000: 0.3572035993077293\n",
      "Loss at epoch 699000: 0.3571394566998641\n",
      "Loss at epoch 700000: 0.3570754604267415\n",
      "Loss at epoch 701000: 0.35701160996581893\n",
      "Loss at epoch 702000: 0.3569479047971295\n",
      "Loss at epoch 703000: 0.35688434440326433\n",
      "Loss at epoch 704000: 0.35682092826935735\n",
      "Loss at epoch 705000: 0.356757655883069\n",
      "Loss at epoch 706000: 0.35669452673456975\n",
      "Loss at epoch 707000: 0.3566315403165257\n",
      "Loss at epoch 708000: 0.3565686961240814\n",
      "Loss at epoch 709000: 0.35650599365484514\n",
      "Loss at epoch 710000: 0.35644343240887333\n",
      "Loss at epoch 711000: 0.3563810118886551\n",
      "Loss at epoch 712000: 0.35631873159909705\n",
      "Loss at epoch 713000: 0.3562565910475085\n",
      "Loss at epoch 714000: 0.3561945897435864\n",
      "Loss at epoch 715000: 0.3561327271994002\n",
      "Loss at epoch 716000: 0.356071002929377\n",
      "Loss at epoch 717000: 0.356009416450288\n",
      "Loss at epoch 718000: 0.35594796728123246\n",
      "Loss at epoch 719000: 0.3558866549436244\n",
      "Loss at epoch 720000: 0.3558254789611779\n",
      "Loss at epoch 721000: 0.355764438859893\n",
      "Loss at epoch 722000: 0.3557035341680412\n",
      "Loss at epoch 723000: 0.35564276441615256\n",
      "Loss at epoch 724000: 0.3555821291370005\n",
      "Loss at epoch 725000: 0.3555216278655889\n",
      "Loss at epoch 726000: 0.3554612601391381\n",
      "Loss at epoch 727000: 0.3554010254970716\n",
      "Loss at epoch 728000: 0.3553409234810021\n",
      "Loss at epoch 729000: 0.35528095363471907\n",
      "Loss at epoch 730000: 0.3552211155041744\n",
      "Loss at epoch 731000: 0.35516140863746964\n",
      "Loss at epoch 732000: 0.3551018325848438\n",
      "Loss at epoch 733000: 0.3550423868986584\n",
      "Loss at epoch 734000: 0.3549830711333868\n",
      "Loss at epoch 735000: 0.3549238848456005\n",
      "Loss at epoch 736000: 0.3548648275939557\n",
      "Loss at epoch 737000: 0.35480589893918213\n",
      "Loss at epoch 738000: 0.35474709844406904\n",
      "Loss at epoch 739000: 0.3546884256734544\n",
      "Loss at epoch 740000: 0.3546298801942114\n",
      "Loss at epoch 741000: 0.35457146157523667\n",
      "Loss at epoch 742000: 0.3545131693874385\n",
      "Loss at epoch 743000: 0.3544550032037239\n",
      "Loss at epoch 744000: 0.3543969625989874\n",
      "Loss at epoch 745000: 0.3543390471500989\n",
      "Loss at epoch 746000: 0.35428125643589226\n",
      "Loss at epoch 747000: 0.35422359003715276\n",
      "Loss at epoch 748000: 0.35416604753660624\n",
      "Loss at epoch 749000: 0.3541086285189079\n",
      "Loss at epoch 750000: 0.35405133257062943\n",
      "Loss at epoch 751000: 0.3539941592802496\n",
      "Loss at epoch 752000: 0.353937108238141\n",
      "Loss at epoch 753000: 0.35388017903656016\n",
      "Loss at epoch 754000: 0.35382337126963626\n",
      "Loss at epoch 755000: 0.3537666845333597\n",
      "Loss at epoch 756000: 0.3537101184255715\n",
      "Loss at epoch 757000: 0.3536536725459525\n",
      "Loss at epoch 758000: 0.35359734649601166\n",
      "Loss at epoch 759000: 0.35354113987907676\n",
      "Loss at epoch 760000: 0.3534850523002832\n",
      "Loss at epoch 761000: 0.35342908336656254\n",
      "Loss at epoch 762000: 0.3533732326866333\n",
      "Loss at epoch 763000: 0.3533174998709897\n",
      "Loss at epoch 764000: 0.3532618845318923\n",
      "Loss at epoch 765000: 0.3532063862833563\n",
      "Loss at epoch 766000: 0.3531510047411425\n",
      "Loss at epoch 767000: 0.353095739522747\n",
      "Loss at epoch 768000: 0.35304059024739065\n",
      "Loss at epoch 769000: 0.35298555653600955\n",
      "Loss at epoch 770000: 0.3529306380112448\n",
      "Loss at epoch 771000: 0.35287583429743297\n",
      "Loss at epoch 772000: 0.3528211450205965\n",
      "Loss at epoch 773000: 0.35276656980843346\n",
      "Loss at epoch 774000: 0.352712108290308\n",
      "Loss at epoch 775000: 0.3526577600972416\n",
      "Loss at epoch 776000: 0.35260352486190233\n",
      "Loss at epoch 777000: 0.3525494022185964\n",
      "Loss at epoch 778000: 0.35249539180325834\n",
      "Loss at epoch 779000: 0.35244149325344176\n",
      "Loss at epoch 780000: 0.3523877062083102\n",
      "Loss at epoch 781000: 0.35233403030862837\n",
      "Loss at epoch 782000: 0.3522804651967519\n",
      "Loss at epoch 783000: 0.3522270105166195\n",
      "Loss at epoch 784000: 0.35217366591374366\n",
      "Loss at epoch 785000: 0.35212043103520124\n",
      "Loss at epoch 786000: 0.35206730552962495\n",
      "Loss at epoch 787000: 0.35201428904719484\n",
      "Loss at epoch 788000: 0.3519613812396291\n",
      "Loss at epoch 789000: 0.3519085817601759\n",
      "Loss at epoch 790000: 0.35185589026360437\n",
      "Loss at epoch 791000: 0.35180330640619595\n",
      "Loss at epoch 792000: 0.3517508298457364\n",
      "Loss at epoch 793000: 0.3516984602415071\n",
      "Loss at epoch 794000: 0.35164619725427615\n",
      "Loss at epoch 795000: 0.3515940405462915\n",
      "Loss at epoch 796000: 0.35154198978127116\n",
      "Loss at epoch 797000: 0.3514900446243957\n",
      "Loss at epoch 798000: 0.3514382047422998\n",
      "Loss at epoch 799000: 0.3513864698030648\n",
      "Loss at epoch 800000: 0.35133483947621\n",
      "Loss at epoch 801000: 0.3512833134326848\n",
      "Loss at epoch 802000: 0.3512318913448613\n",
      "Loss at epoch 803000: 0.3511805728865254\n",
      "Loss at epoch 804000: 0.35112935773286996\n",
      "Loss at epoch 805000: 0.3510782455604869\n",
      "Loss at epoch 806000: 0.35102723604735864\n",
      "Loss at epoch 807000: 0.3509763288728516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 808000: 0.35092552371770774\n",
      "Loss at epoch 809000: 0.35087482026403766\n",
      "Loss at epoch 810000: 0.350824218195312\n",
      "Loss at epoch 811000: 0.35077371719635553\n",
      "Loss at epoch 812000: 0.35072331695333847\n",
      "Loss at epoch 813000: 0.35067301715376975\n",
      "Loss at epoch 814000: 0.35062281748648944\n",
      "Loss at epoch 815000: 0.3505727176416617\n",
      "Loss at epoch 816000: 0.3505227173107675\n",
      "Loss at epoch 817000: 0.35047281618659715\n",
      "Loss at epoch 818000: 0.3504230139632437\n",
      "Loss at epoch 819000: 0.35037331033609576\n",
      "Loss at epoch 820000: 0.35032370500182997\n",
      "Loss at epoch 821000: 0.3502741976584046\n",
      "Loss at epoch 822000: 0.35022478800505236\n",
      "Loss at epoch 823000: 0.3501754757422738\n",
      "Loss at epoch 824000: 0.35012626057183016\n",
      "Loss at epoch 825000: 0.35007714219673647\n",
      "Loss at epoch 826000: 0.35002812032125497\n",
      "Loss at epoch 827000: 0.349979194650889\n",
      "Loss at epoch 828000: 0.3499303648923752\n",
      "Loss at epoch 829000: 0.3498816307536777\n",
      "Loss at epoch 830000: 0.34983299194398154\n",
      "Loss at epoch 831000: 0.34978444817368554\n",
      "Loss at epoch 832000: 0.3497359991543965\n",
      "Loss at epoch 833000: 0.3496876445989223\n",
      "Loss at epoch 834000: 0.34963938422126606\n",
      "Loss at epoch 835000: 0.3495912177366186\n",
      "Loss at epoch 836000: 0.34954314486135374\n",
      "Loss at epoch 837000: 0.34949516531302066\n",
      "Loss at epoch 838000: 0.34944727881033866\n",
      "Loss at epoch 839000: 0.34939948507318996\n",
      "Loss at epoch 840000: 0.3493517838226148\n",
      "Loss at epoch 841000: 0.349304174780804\n",
      "Loss at epoch 842000: 0.34925665767109354\n",
      "Loss at epoch 843000: 0.3492092322179588\n",
      "Loss at epoch 844000: 0.34916189814700854\n",
      "Loss at epoch 845000: 0.34911465518497753\n",
      "Loss at epoch 846000: 0.34906750305972284\n",
      "Loss at epoch 847000: 0.34902044150021655\n",
      "Loss at epoch 848000: 0.3489734702365401\n",
      "Loss at epoch 849000: 0.3489265889998788\n",
      "Loss at epoch 850000: 0.3488797975225158\n",
      "Loss at epoch 851000: 0.34883309553782627\n",
      "Loss at epoch 852000: 0.3487864827802719\n",
      "Loss at epoch 853000: 0.34873995898539567\n",
      "Loss at epoch 854000: 0.3486935238898154\n",
      "Loss at epoch 855000: 0.34864717723121835\n",
      "Loss at epoch 856000: 0.34860091874835625\n",
      "Loss at epoch 857000: 0.3485547481810391\n",
      "Loss at epoch 858000: 0.3485086652701299\n",
      "Loss at epoch 859000: 0.34846266975753987\n",
      "Loss at epoch 860000: 0.3484167613862218\n",
      "Loss at epoch 861000: 0.3483709399001653\n",
      "Loss at epoch 862000: 0.3483252050443921\n",
      "Loss at epoch 863000: 0.3482795565649495\n",
      "Loss at epoch 864000: 0.3482339942089061\n",
      "Loss at epoch 865000: 0.3481885177243457\n",
      "Loss at epoch 866000: 0.34814312686036314\n",
      "Loss at epoch 867000: 0.3480978213670582\n",
      "Loss at epoch 868000: 0.3480526009955308\n",
      "Loss at epoch 869000: 0.3480074654978759\n",
      "Loss at epoch 870000: 0.3479624146271785\n",
      "Loss at epoch 871000: 0.3479174481375083\n",
      "Loss at epoch 872000: 0.34787256578391496\n",
      "Loss at epoch 873000: 0.34782776732242326\n",
      "Loss at epoch 874000: 0.34778305251002745\n",
      "Loss at epoch 875000: 0.3477384211046872\n",
      "Loss at epoch 876000: 0.347693872865322\n",
      "Loss at epoch 877000: 0.3476494075518067\n",
      "Loss at epoch 878000: 0.34760502492496687\n",
      "Loss at epoch 879000: 0.3475607247465733\n",
      "Loss at epoch 880000: 0.34751650677933765\n",
      "Loss at epoch 881000: 0.34747237078690796\n",
      "Loss at epoch 882000: 0.3474283165338631\n",
      "Loss at epoch 883000: 0.34738434378570926\n",
      "Loss at epoch 884000: 0.34734045230887434\n",
      "Loss at epoch 885000: 0.3472966418707035\n",
      "Loss at epoch 886000: 0.34725291223945537\n",
      "Loss at epoch 887000: 0.3472092631842961\n",
      "Loss at epoch 888000: 0.347165694475296\n",
      "Loss at epoch 889000: 0.34712220588342424\n",
      "Loss at epoch 890000: 0.3470787971805449\n",
      "Loss at epoch 891000: 0.3470354681394123\n",
      "Loss at epoch 892000: 0.3469922185336663\n",
      "Loss at epoch 893000: 0.3469490481378285\n",
      "Loss at epoch 894000: 0.34690595672729735\n",
      "Loss at epoch 895000: 0.346862944078344\n",
      "Loss at epoch 896000: 0.346820009968108\n",
      "Loss at epoch 897000: 0.34677715417459265\n",
      "Loss at epoch 898000: 0.3467343764766615\n",
      "Loss at epoch 899000: 0.3466916766540332\n",
      "Loss at epoch 900000: 0.3466490544872778\n",
      "Loss at epoch 901000: 0.34660650975781254\n",
      "Loss at epoch 902000: 0.34656404224789766\n",
      "Loss at epoch 903000: 0.3465216517406318\n",
      "Loss at epoch 904000: 0.34647933801994885\n",
      "Loss at epoch 905000: 0.3464371008706127\n",
      "Loss at epoch 906000: 0.34639494007821414\n",
      "Loss at epoch 907000: 0.3463528554291661\n",
      "Loss at epoch 908000: 0.3463108467106999\n",
      "Loss at epoch 909000: 0.3462689137108618\n",
      "Loss at epoch 910000: 0.34622705621850786\n",
      "Loss at epoch 911000: 0.3461852740233011\n",
      "Loss at epoch 912000: 0.34614356691570675\n",
      "Loss at epoch 913000: 0.3461019346869892\n",
      "Loss at epoch 914000: 0.34606037712920684\n",
      "Loss at epoch 915000: 0.34601889403520986\n",
      "Loss at epoch 916000: 0.34597748519863497\n",
      "Loss at epoch 917000: 0.3459361504139022\n",
      "Loss at epoch 918000: 0.34589488947621116\n",
      "Loss at epoch 919000: 0.34585370218153727\n",
      "Loss at epoch 920000: 0.34581258832662803\n",
      "Loss at epoch 921000: 0.34577154770899887\n",
      "Loss at epoch 922000: 0.34573058012692964\n",
      "Loss at epoch 923000: 0.34568968537946154\n",
      "Loss at epoch 924000: 0.34564886326639294\n",
      "Loss at epoch 925000: 0.34560811358827526\n",
      "Loss at epoch 926000: 0.3455674361464102\n",
      "Loss at epoch 927000: 0.34552683074284624\n",
      "Loss at epoch 928000: 0.345486297180374\n",
      "Loss at epoch 929000: 0.34544583526252365\n",
      "Loss at epoch 930000: 0.3454054447935612\n",
      "Loss at epoch 931000: 0.3453651255784845\n",
      "Loss at epoch 932000: 0.34532487742302015\n",
      "Loss at epoch 933000: 0.3452847001336206\n",
      "Loss at epoch 934000: 0.3452445935174594\n",
      "Loss at epoch 935000: 0.3452045573824285\n",
      "Loss at epoch 936000: 0.3451645915371353\n",
      "Loss at epoch 937000: 0.34512469579089844\n",
      "Loss at epoch 938000: 0.34508486995374477\n",
      "Loss at epoch 939000: 0.34504511383640607\n",
      "Loss at epoch 940000: 0.3450054272503154\n",
      "Loss at epoch 941000: 0.34496581000760446\n",
      "Loss at epoch 942000: 0.34492626192109943\n",
      "Loss at epoch 943000: 0.3448867828043182\n",
      "Loss at epoch 944000: 0.34484737247146763\n",
      "Loss at epoch 945000: 0.3448080307374388\n",
      "Loss at epoch 946000: 0.3447687574178054\n",
      "Loss at epoch 947000: 0.3447295523288193\n",
      "Loss at epoch 948000: 0.3446904152874086\n",
      "Loss at epoch 949000: 0.3446513461111735\n",
      "Loss at epoch 950000: 0.344612344618383\n",
      "Loss at epoch 951000: 0.3445734106279729\n",
      "Loss at epoch 952000: 0.3445345439595416\n",
      "Loss at epoch 953000: 0.3444957444333478\n",
      "Loss at epoch 954000: 0.3444570118703062\n",
      "Loss at epoch 955000: 0.3444183460919864\n",
      "Loss at epoch 956000: 0.344379746920608\n",
      "Loss at epoch 957000: 0.3443412141790386\n",
      "Loss at epoch 958000: 0.3443027476907904\n",
      "Loss at epoch 959000: 0.34426434728001765\n",
      "Loss at epoch 960000: 0.344226012771513\n",
      "Loss at epoch 961000: 0.3441877439907052\n",
      "Loss at epoch 962000: 0.34414954076365556\n",
      "Loss at epoch 963000: 0.34411140291705566\n",
      "Loss at epoch 964000: 0.344073330278224\n",
      "Loss at epoch 965000: 0.34403532267510334\n",
      "Loss at epoch 966000: 0.3439973799362577\n",
      "Loss at epoch 967000: 0.34395950189086943\n",
      "Loss at epoch 968000: 0.34392168836873693\n",
      "Loss at epoch 969000: 0.3438839392002707\n",
      "Loss at epoch 970000: 0.3438462542164917\n",
      "Loss at epoch 971000: 0.34380863324902794\n",
      "Loss at epoch 972000: 0.3437710761301119\n",
      "Loss at epoch 973000: 0.34373358269257764\n",
      "Loss at epoch 974000: 0.34369615276985815\n",
      "Loss at epoch 975000: 0.34365878619598283\n",
      "Loss at epoch 976000: 0.3436214828055742\n",
      "Loss at epoch 977000: 0.3435842424338455\n",
      "Loss at epoch 978000: 0.34354706491659853\n",
      "Loss at epoch 979000: 0.34350995009022006\n",
      "Loss at epoch 980000: 0.3434728977916799\n",
      "Loss at epoch 981000: 0.3434359078585276\n",
      "Loss at epoch 982000: 0.3433989801288908\n",
      "Loss at epoch 983000: 0.3433621144414713\n",
      "Loss at epoch 984000: 0.34332531063554395\n",
      "Loss at epoch 985000: 0.3432885685509528\n",
      "Loss at epoch 986000: 0.3432518880281092\n",
      "Loss at epoch 987000: 0.3432152689079889\n",
      "Loss at epoch 988000: 0.34317871103212993\n",
      "Loss at epoch 989000: 0.3431422142426299\n",
      "Loss at epoch 990000: 0.34310577838214323\n",
      "Loss at epoch 991000: 0.3430694032938788\n",
      "Loss at epoch 992000: 0.34303308882159783\n",
      "Loss at epoch 993000: 0.3429968348096108\n",
      "Loss at epoch 994000: 0.34296064110277535\n",
      "Loss at epoch 995000: 0.34292450754649395\n",
      "Loss at epoch 996000: 0.3428884339867112\n",
      "Loss at epoch 997000: 0.34285242026991136\n",
      "Loss at epoch 998000: 0.34281646624311635\n",
      "Loss at epoch 999000: 0.3427805717538829\n"
     ]
    }
   ],
   "source": [
    "W = logictisRegression(X_train, y_train, learning_rate=0.0001, epoch = 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code train accuracy: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "X_train_bar = np.concatenate([np.ones([X_train.shape[0],1]),X_train],axis = 1)\n",
    "y_pred = sigmoid(W, X_train_bar)\n",
    "y_pred = np.where(y_pred>0.5,1,0)\n",
    "print(\"Code train accuracy: {}\".format(np.mean(y_train==y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code test accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "X_test_bar = np.concatenate([np.ones([X_test.shape[0],1]),X_test],axis = 1)\n",
    "y_pred = sigmoid(W, X_test_bar)\n",
    "y_pred = np.where(y_pred>0.5,1,0)\n",
    "print(\"Code test accuracy: {}\".format(np.mean(y_test==y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhvu/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Kiểm tra lại\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bài 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X, y, test_size=0.33, random_state=42)\n",
    "#Normalize\n",
    "max = np.max(X_train, axis=0)\n",
    "min = np.min(X_train, axis=0)\n",
    "X_train = (X_train - min) / (max -min)\n",
    "X_test = (X_test - min) / (max - min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train bar\n",
    "X_train_bar=np.concatenate([np.ones([X_train.shape[0],1]),X_train], axis=1)\n",
    "#y_train onehot\n",
    "y_train_onehot = np.zeros((y_train.size, y_train.max() + 1), dtype=int)\n",
    "y_train_onehot[np.arange(y_train.size), y_train.reshape(-1)] = 1\n",
    "#X_test_bar\n",
    "X_test_bar = np.concatenate([np.ones([X_test.shape[0],1]), X_test], axis=1)\n",
    "#y_test onehot\n",
    "y_test_onehot = np.zeros((y_test.size, y_test.max() + 1), dtype=int)\n",
    "y_test_onehot[np.arange(y_test.size), y_test.reshape(-1)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add one to X\n",
    "X_train_bar = np.concatenate([np.ones([X_train.shape[0],1]),X_train], axis=1)\n",
    "X_test_bar = np.concatenate([np.ones([X_test.shape[0],1]),X_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train_bar\n",
    "y = y_train_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(W , X, y, a):\n",
    "    W_temp = np.zeros([X.shape[1], y.shape[1]])\n",
    "    for i in range(y.shape[1]):\n",
    "        temp = i + 1\n",
    "        W_temp[:, i:temp] = np.mean(a[:, i:temp] * X, axis=0).reshape(-1, 1)\n",
    "    return W_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(W, learning_rate, epochs):\n",
    "    global X, y\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        expi = np.exp(X.dot(W))\n",
    "        expsum = np.sum(expi, axis=1)\n",
    "        soft1 = expi / expsum.reshape(-1, 1)\n",
    "        a = -y + soft1\n",
    "        W = W - learning_rate * grad(W,X,y,a)\n",
    "\n",
    "    print(f'Final W: {W}')\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([[1],\n",
    "              [1],\n",
    "              [1],\n",
    "              [1],\n",
    "              [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final W: [[ 2.52887093  2.07444114 -1.60731207]\n",
      " [-0.34476822  1.16915554  2.17383033]\n",
      " [ 3.34514848 -0.69112068  0.34413401]\n",
      " [-1.93574407  1.79475293  3.13905114]\n",
      " [-1.7577722   0.87812648  3.87778072]]\n"
     ]
    }
   ],
   "source": [
    "W0 = softmax(W, learning_rate = 0.002, epochs = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.98851932e-02, 5.99050628e-01, 3.51064179e-01],\n",
       "       [9.16088748e-01, 7.41445168e-02, 9.76673512e-03],\n",
       "       [2.89649475e-04, 1.48350687e-01, 8.51359663e-01],\n",
       "       [4.49346340e-02, 5.22744122e-01, 4.32321244e-01],\n",
       "       [2.29703194e-02, 5.01593102e-01, 4.75436579e-01],\n",
       "       [8.62330835e-01, 1.24266016e-01, 1.34031495e-02],\n",
       "       [1.37553275e-01, 5.90272017e-01, 2.72174708e-01],\n",
       "       [4.87819469e-03, 2.10776517e-01, 7.84345288e-01],\n",
       "       [1.31709181e-02, 6.04992622e-01, 3.81836459e-01],\n",
       "       [8.66827014e-02, 6.42157344e-01, 2.71159955e-01],\n",
       "       [1.34738736e-02, 2.91696223e-01, 6.94829904e-01],\n",
       "       [8.60280035e-01, 1.33039259e-01, 6.68070583e-03],\n",
       "       [9.12626802e-01, 8.04215944e-02, 6.95160341e-03],\n",
       "       [8.68172144e-01, 1.24892421e-01, 6.93543466e-03],\n",
       "       [9.43454820e-01, 5.11650304e-02, 5.38014978e-03],\n",
       "       [5.07768460e-02, 4.13269216e-01, 5.35953938e-01],\n",
       "       [3.74012873e-03, 2.32057305e-01, 7.64202566e-01],\n",
       "       [7.95815900e-02, 6.97436808e-01, 2.22981602e-01],\n",
       "       [6.12081868e-02, 6.00877591e-01, 3.37914222e-01],\n",
       "       [3.52238260e-03, 2.64059989e-01, 7.32417629e-01],\n",
       "       [8.77962678e-01, 1.14731064e-01, 7.30625798e-03],\n",
       "       [2.19074293e-02, 4.02510365e-01, 5.75582205e-01],\n",
       "       [8.75998245e-01, 1.12917446e-01, 1.10843083e-02],\n",
       "       [4.29951006e-03, 2.88793135e-01, 7.06907355e-01],\n",
       "       [4.14318640e-03, 1.32292249e-01, 8.63564564e-01],\n",
       "       [4.42989081e-03, 2.24848445e-01, 7.70721664e-01],\n",
       "       [3.40592540e-03, 3.72580376e-01, 6.24013699e-01],\n",
       "       [3.04347312e-03, 1.78387247e-01, 8.18569280e-01],\n",
       "       [8.29836935e-01, 1.59856055e-01, 1.03070107e-02],\n",
       "       [8.51488401e-01, 1.39765575e-01, 8.74602410e-03],\n",
       "       [9.58927812e-01, 3.85791560e-02, 2.49303158e-03],\n",
       "       [9.69688846e-01, 2.54998995e-02, 4.81125402e-03],\n",
       "       [5.23246516e-02, 4.82828898e-01, 4.64846450e-01],\n",
       "       [9.07864398e-01, 8.59422333e-02, 6.19336843e-03],\n",
       "       [9.10163451e-01, 8.52153318e-02, 4.62121678e-03],\n",
       "       [6.92479747e-03, 4.15231919e-01, 5.77843283e-01],\n",
       "       [5.67827560e-02, 4.55239777e-01, 4.87977467e-01],\n",
       "       [9.12907305e-01, 8.04182037e-02, 6.67449170e-03],\n",
       "       [9.36524131e-01, 5.87707551e-02, 4.70511421e-03],\n",
       "       [9.72004477e-01, 2.55167943e-02, 2.47872897e-03],\n",
       "       [1.18253717e-02, 4.20322546e-01, 5.67852083e-01],\n",
       "       [8.10383170e-02, 4.20439478e-01, 4.98522205e-01],\n",
       "       [3.53079459e-02, 4.44068990e-01, 5.20623064e-01],\n",
       "       [9.46114300e-01, 4.76833848e-02, 6.20231542e-03],\n",
       "       [9.31816382e-01, 6.21693912e-02, 6.01422709e-03],\n",
       "       [9.20442645e-02, 7.26891474e-01, 1.81064262e-01],\n",
       "       [2.06004473e-02, 4.90752743e-01, 4.88646810e-01],\n",
       "       [1.30929822e-02, 3.41306958e-01, 6.45600060e-01],\n",
       "       [4.74873322e-02, 5.03506856e-01, 4.49005812e-01],\n",
       "       [2.21872969e-03, 1.06168137e-01, 8.91613134e-01]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expi = np.exp(X_test_bar.dot(W0))\n",
    "expsum = np.sum(expi, axis=1)  \n",
    "soft1 = expi / expsum.reshape(-1,1)\n",
    "soft1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Kết quả\n",
    "np.mean(y_test.reshape(-1) == np.argmax(expi / expsum.reshape(-1,1), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dùng sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of lab_03.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
